{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad36b066-75fe-461d-b2b8-147de46a9592",
   "metadata": {},
   "source": [
    "# Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be87670e-9018-4361-8b73-ad08a403ba1d",
   "metadata": {},
   "source": [
    "* https://www.youtube.com/watch?v=PVpBGs_iSjY&list=PL3MmuxUbc_hIB4fSqLy_0AfTjVLpgjV3R\n",
    "* https://github.com/DataTalksClub/llm-zoomcamp/tree/main/02-open-source#27-ollama---running-llms-on-a-cpu\n",
    "* https://github.com/ollama/ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "955b48cd-c0ce-4ab8-8ff2-9c6a9ac57c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.35.10-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from openai) (4.4.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.10/site-packages (from openai) (0.27.0)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /home/codespace/.local/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m167.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions<5,>=4.7 in /home/codespace/.local/lib/python3.10/site-packages (from openai) (4.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Downloading openai-1.35.10-py3-none-any.whl (328 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.3/328.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.9/423.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m169.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: tqdm, pydantic-core, distro, annotated-types, pydantic, openai\n",
      "Successfully installed annotated-types-0.7.0 distro-1.9.0 openai-1.35.10 pydantic-2.8.2 pydantic-core-2.20.1 tqdm-4.66.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62048e07-e5f4-48ac-afce-f53190b0f634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Downloading ollama...\n",
      "######################################################################## 100.0%#=#=#                                                                         \n",
      ">>> Installing ollama to /usr/local/bin...\n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n"
     ]
    }
   ],
   "source": [
    "# Install Ollama\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69127e59-e6ce-4efe-a6dd-d763cbe3d4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find '/home/codespace/.ollama/id_ed25519'. Generating new private key.\n",
      "Your new public key is: \n",
      "\n",
      "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIOWFmzQQFnK7srocy1s0jl40oNIpN/CGEopuyatfWUI\n",
      "\n",
      "2024/07/08 18:09:41 routes.go:1064: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE: OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512 OLLAMA_MAX_VRAM:0 OLLAMA_MODELS:/home/codespace/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]\"\n",
      "time=2024-07-08T18:09:41.694Z level=INFO source=images.go:730 msg=\"total blobs: 0\"\n",
      "time=2024-07-08T18:09:41.695Z level=INFO source=images.go:737 msg=\"total unused blobs removed: 0\"\n",
      "time=2024-07-08T18:09:41.695Z level=INFO source=routes.go:1111 msg=\"Listening on 127.0.0.1:11434 (version 0.1.48)\"\n",
      "time=2024-07-08T18:09:41.695Z level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/tmp/ollama4241835502/runners\n",
      "time=2024-07-08T18:09:45.066Z level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [cpu_avx2 cuda_v11 rocm_v60101 cpu cpu_avx]\"\n",
      "time=2024-07-08T18:09:45.168Z level=INFO source=types.go:98 msg=\"inference compute\" id=0 library=cpu compute=\"\" driver=0.0 name=\"\" total=\"15.6 GiB\" available=\"13.7 GiB\"\n",
      "[GIN] 2024/07/08 - 18:12:47 |\u001b[97;42m 200 \u001b[0m|      49.994µs |       127.0.0.1 |\u001b[97;45m HEAD    \u001b[0m \"/\"\n",
      "[GIN] 2024/07/08 - 18:12:47 |\u001b[97;42m 200 \u001b[0m|       91.13µs |       127.0.0.1 |\u001b[97;44m GET     \u001b[0m \"/api/ps\"\n",
      "[GIN] 2024/07/08 - 18:13:07 |\u001b[97;42m 200 \u001b[0m|      20.287µs |       127.0.0.1 |\u001b[97;45m HEAD    \u001b[0m \"/\"\n",
      "[GIN] 2024/07/08 - 18:13:07 |\u001b[90;43m 404 \u001b[0m|     160.199µs |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/show\"\n",
      "time=2024-07-08T18:13:09.358Z level=INFO source=download.go:136 msg=\"downloading 3e38718d00bb in 22 100 MB part(s)\"\n",
      "time=2024-07-08T18:13:24.529Z level=INFO source=download.go:136 msg=\"downloading fa8235e5b48f in 1 1.1 KB part(s)\"\n",
      "time=2024-07-08T18:13:27.587Z level=INFO source=download.go:136 msg=\"downloading 542b217f179c in 1 148 B part(s)\"\n",
      "time=2024-07-08T18:13:30.605Z level=INFO source=download.go:136 msg=\"downloading 8dde1baf1db0 in 1 78 B part(s)\"\n",
      "time=2024-07-08T18:13:33.689Z level=INFO source=download.go:136 msg=\"downloading ed7ab7698fdd in 1 483 B part(s)\"\n",
      "[GIN] 2024/07/08 - 18:13:37 |\u001b[97;42m 200 \u001b[0m| 29.956282582s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/pull\"\n",
      "[GIN] 2024/07/08 - 18:13:37 |\u001b[97;42m 200 \u001b[0m|    5.007993ms |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/show\"\n",
      "time=2024-07-08T18:13:37.346Z level=INFO source=memory.go:309 msg=\"offload to cpu\" layers.requested=-1 layers.model=33 layers.offload=0 layers.split=\"\" memory.available=\"[13.7 GiB]\" memory.required.full=\"2.9 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"768.0 MiB\" memory.required.allocations=\"[2.9 GiB]\" memory.weights.total=\"2.6 GiB\" memory.weights.repeating=\"2.6 GiB\" memory.weights.nonrepeating=\"77.1 MiB\" memory.graph.full=\"128.0 MiB\" memory.graph.partial=\"128.0 MiB\"\n",
      "time=2024-07-08T18:13:37.346Z level=INFO source=server.go:368 msg=\"starting llama server\" cmd=\"/tmp/ollama4241835502/runners/cpu_avx2/ollama_llama_server --model /home/codespace/.ollama/models/blobs/sha256-3e38718d00bb0007ab7c0cb4a038e7718c07b54f486a7810efd03bb4e894592a --ctx-size 2048 --batch-size 512 --embedding --log-disable --parallel 1 --port 38047\"\n",
      "time=2024-07-08T18:13:37.347Z level=INFO source=sched.go:382 msg=\"loaded runners\" count=1\n",
      "time=2024-07-08T18:13:37.347Z level=INFO source=server.go:556 msg=\"waiting for llama runner to start responding\"\n",
      "time=2024-07-08T18:13:37.347Z level=INFO source=server.go:594 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
      "INFO [main] build info | build=1 commit=\"7c26775\" tid=\"129976845043584\" timestamp=1720462417\n",
      "INFO [main] system info | n_threads=2 n_threads_batch=-1 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"129976845043584\" timestamp=1720462417 total_threads=4\n",
      "INFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"3\" port=\"38047\" tid=\"129976845043584\" timestamp=1720462417\n",
      "llama_model_loader: loaded meta data with 27 key-value pairs and 197 tensors from /home/codespace/.ollama/models/blobs/sha256-3e38718d00bb0007ab7c0cb4a038e7718c07b54f486a7810efd03bb4e894592a (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
      "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
      "llama_model_loader: - kv   2:                        phi3.context_length u32              = 131072\n",
      "llama_model_loader: - kv   3:  phi3.rope.scaling.original_context_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                      phi3.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   5:                   phi3.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   6:                           phi3.block_count u32              = 32\n",
      "llama_model_loader: - kv   7:                  phi3.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:               phi3.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                  phi3.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv  11:                        phi3.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  13:              phi3.rope.scaling.attn_factor f32              = 1.190238\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 32000\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
      "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   67 tensors\n",
      "llama_model_loader: - type q4_0:  129 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens cache size = 323\n",
      "llm_load_vocab: token to piece cache size = 0.1687 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = phi3\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32064\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 96\n",
      "llm_load_print_meta: n_embd_head_k    = 96\n",
      "llm_load_print_meta: n_embd_head_v    = 96\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
      "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 3.82 B\n",
      "llm_load_print_meta: model size       = 2.03 GiB (4.55 BPW) \n",
      "llm_load_print_meta: general.name     = Phi3\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  2074.66 MiB\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "time=2024-07-08T18:13:37.598Z level=INFO source=server.go:594 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.13 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   168.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "INFO [main] model loaded | tid=\"129976845043584\" timestamp=1720462418\n",
      "time=2024-07-08T18:13:38.351Z level=INFO source=server.go:599 msg=\"llama runner started in 1.00 seconds\"\n",
      "[GIN] 2024/07/08 - 18:13:38 |\u001b[97;42m 200 \u001b[0m|  1.010159139s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/chat\"\n",
      "[GIN] 2024/07/08 - 18:18:31 |\u001b[97;42m 200 \u001b[0m|         1m24s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/chat\"\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# In the Powershell\n",
    "!ollama start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "386ab2ff-bec3-4a71-9b7f-67f95bc66f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: could not connect to ollama app, is it running?\n"
     ]
    }
   ],
   "source": [
    "# In the Powershell\n",
    "!ollama run phi3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8147f94-4266-440d-988a-2c623edee12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-08 18:20:50--  https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3832 (3.7K) [text/plain]\n",
      "Saving to: ‘minsearch.py’\n",
      "\n",
      "minsearch.py        100%[===================>]   3.74K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-07-08 18:20:50 (32.2 MB/s) - ‘minsearch.py’ saved [3832/3832]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -f minsearch.py\n",
    "!wget https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "897364f7-c828-4381-a2ab-0599847ae1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x7dc24a20a620>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "import minsearch\n",
    "\n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)\n",
    "\n",
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc8642d2-584e-4368-aa4e-8d080fb3872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80d105a6-8a97-4f73-8a90-56056785c481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78c5d2a3-bf86-4a6a-bd04-bee6d65f8ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://youtu.be/PVpBGs_iSjY?list=PL3MmuxUbc_hIB4fSqLy_0AfTjVLpgjV3R&t=48\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f86aa4e-e0cb-40da-8765-3767be01a757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model='phi3',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92114e9b-ebd0-455e-814b-d8ad9bad16b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdac6116-0b54-41e7-95b0-36c48b61ca6d",
   "metadata": {},
   "source": [
    "## Running Ollama in Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f93c75c-c46d-4cff-a1d8-5714d99e36fd",
   "metadata": {},
   "source": [
    "* https://github.com/DataTalksClub/llm-zoomcamp/tree/main/02-open-source#27-ollama---running-llms-on-a-cpu\n",
    "* https://hub.docker.com/r/ollama/ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00260f7-8f7f-45a2-8edc-6adcc122d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the Powershell\n",
    "docker run -it \\\n",
    "    -v ollama:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef68b2-54a3-4ca3-8409-1c5d49540dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run Docker image in the interactive mode\n",
    "docker exec -it ollama bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d62a0-5973-4478-b8d2-9935089cde67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the Docker container upload LLM model\n",
    "ollama pull phi3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

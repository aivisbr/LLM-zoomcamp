{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23da9fcf-8efa-49d7-abda-090be69cada0",
   "metadata": {},
   "source": [
    "# Homework 2: Open-Source LLMs for LLM Zoomcamp 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57a95705-43c7-4182-b256-8d6041e91743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/python/3.10.13/lib/python3.10/site-packages (1.35.10)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m885.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.10/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/codespace/.local/lib/python3.10/site-packages (from openai) (4.12.0)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (3.14.0)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests->transformers) (2.0.7)\n",
      "Downloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.23.4 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.42.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openai transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13aff0e4-b84e-4e32-b3d1-db8a8c63fa04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024/07/09 13:33:14 routes.go:1064: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE: OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512 OLLAMA_MAX_VRAM:0 OLLAMA_MODELS:/root/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]\"\n",
      "time=2024-07-09T13:33:14.309Z level=INFO source=images.go:730 msg=\"total blobs: 10\"\n",
      "time=2024-07-09T13:33:14.335Z level=INFO source=images.go:737 msg=\"total unused blobs removed: 0\"\n",
      "time=2024-07-09T13:33:14.335Z level=INFO source=routes.go:1111 msg=\"Listening on [::]:11434 (version 0.1.48)\"\n",
      "time=2024-07-09T13:33:14.336Z level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/tmp/ollama2515061584/runners\n",
      "time=2024-07-09T13:33:17.719Z level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [cuda_v11 rocm_v60101 cpu cpu_avx cpu_avx2]\"\n",
      "time=2024-07-09T13:33:17.721Z level=INFO source=types.go:98 msg=\"inference compute\" id=0 library=cpu compute=\"\" driver=0.0 name=\"\" total=\"15.6 GiB\" available=\"13.9 GiB\"\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Run Ollama using Docker\n",
    "!docker run -it \\\n",
    "    --rm \\\n",
    "    -v ollama:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14d55c41-bec2-4db0-88e4-947b3dfa6984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama version is 0.1.48\n"
     ]
    }
   ],
   "source": [
    "# Ollama version\n",
    "!docker exec -it ollama ollama -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6773049-e57c-447b-9e4b-dd6ff4854cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Gemma 2B model\n",
    "# https://ollama.com/library/gemma\n",
    "ollama pull gemma:2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7440dfce-42d7-45ab-bf12-ad2758717e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"schemaVersion\":2,\"mediaType\":\"application/vnd.docker.distribution.manifest.v2+json\",\"config\":{\"mediaType\":\"application/vnd.docker.container.image.v1+json\",\"digest\":\"sha256:887433b89a901c156f7e6944442f3c9e57f3c55d6ed52042cbb7303aea994290\",\"size\":483},\"layers\":[{\"mediaType\":\"application/vnd.ollama.image.model\",\"digest\":\"sha256:c1864a5eb19305c40519da12cc543519e48a0697ecd30e15d5ac228644957d12\",\"size\":1678447520},{\"mediaType\":\"application/vnd.ollama.image.license\",\"digest\":\"sha256:097a36493f718248845233af1d3fefe7a303f864fae13bc31a3a9704229378ca\",\"size\":8433},{\"mediaType\":\"application/vnd.ollama.image.template\",\"digest\":\"sha256:109037bec39c0becc8221222ae23557559bc594290945a2c4221ab4f303b8871\",\"size\":136},{\"mediaType\":\"application/vnd.ollama.image.params\",\"digest\":\"sha256:22a838ceb7fb22755a3b0ae9b4eadde629d19be1f651f73efb8c6b4e2cd0eea0\",\"size\":84}]}"
     ]
    }
   ],
   "source": [
    "# Q2 Manifest file\n",
    "# Manifest content of the Gemma 2B model\n",
    "!docker exec -it ollama bash -c 'cat /root/.ollama/models/manifests/registry.ollama.ai/library/gemma/2b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90f89560-f74f-4302-aa0d-211241e2c8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[?25l\u001b[2K\u001b[1G\u001b[?25h\u001b[2K\u001b[1G\u001b[?25h\u001b[?25l\u001b[?25h\u001b[?2004h>>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m\u001b[K\n",
      "Use Ctrl + d or /bye to exit.\n",
      ">>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m\u001b[K\n",
      ">>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Run the Gemma 2B model\n",
    "!docker exec -it ollama bash -c 'ollama run gemma:2b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a643921-1aa1-47ac-b52f-3fd7100293b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dbec690-e4f7-4581-8cf5-72f3c729a8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt, temperature = None):\n",
    "    response = client.chat.completions.create(\n",
    "        model='gemma:2b',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85e40767-8f00-4c93-bee1-8a332895a623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, here is the model you requested:\\n\\n10 * 10'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3. Running the LLM. Output from 10 * 10\n",
    "llm(\"10 * 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c42ee0f-9c9b-4db0-aacc-32cd5462b908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6G\t./ollama_files/models/blobs\n",
      "8.0K\t./ollama_files/models/manifests/registry.ollama.ai/library/gemma\n",
      "12K\t./ollama_files/models/manifests/registry.ollama.ai/library\n",
      "16K\t./ollama_files/models/manifests/registry.ollama.ai\n",
      "20K\t./ollama_files/models/manifests\n",
      "1.6G\t./ollama_files/models\n",
      "1.6G\t./ollama_files\n"
     ]
    }
   ],
   "source": [
    "# Q4. Downloading the weights. Size of the folder\n",
    "!du -h | grep ollama_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee35dc4f-625f-43e4-bf91-e5acc282c58f",
   "metadata": {},
   "source": [
    "**Q5. Adding the weights. Dockerfile** \\\n",
    "FROM ollama/ollama\\\n",
    "**COPY ollama_files /root/.ollama**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f1daadb-51f5-44aa-8b35-e5eef65527f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Serving the LLM. Number of output tokens\n",
    "# https://github.com/DataTalksClub/llm-zoomcamp/blob/main/cohorts/2024/02-open-source/homework.md#q6-serving-it\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02f059a0-377a-4f7d-b747-561e765c8e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = llm(\"What's the formula for energy?\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e12e5f2-22c4-4d4f-9985-05a193f49b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure, here's the formula for energy:\n",
       "\n",
       "**E = K + U**\n",
       "\n",
       "Where:\n",
       "\n",
       "* **E** is the energy in joules (J)\n",
       "* **K** is the kinetic energy in joules (J)\n",
       "* **U** is the potential energy in joules (J)\n",
       "\n",
       "**Kinetic energy (K)** is the energy an object possesses when it moves or is in motion. It is calculated as half the product of an object's mass (m) and its velocity (v) squared:\n",
       "\n",
       "**K = 1/2 * m * v^2**\n",
       "\n",
       "**Potential energy (U)** is the energy an object possesses when it is in a position or has a specific configuration. It is calculated as the product of an object's mass and the gravitational constant (g) multiplied by the height or distance of the object from a reference point.\n",
       "\n",
       "**Gravitational potential energy (U)** is given by the formula:\n",
       "\n",
       "**U = mgh**\n",
       "\n",
       "Where:\n",
       "\n",
       "* **m** is the mass of the object in kilograms (kg)\n",
       "* **g** is the acceleration due to gravity in meters per second squared (m/s^2)\n",
       "* **h** is the height or distance of the object in meters (m)\n",
       "\n",
       "The formula for energy can be used to calculate the total energy of an object, the energy of a specific part of an object, or the change in energy of an object over time."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "878d66bc-e449-44cf-aa1c-db8291e77350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ca52b738-28b2-4268-98dd-540723e93bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/google/gemma-2b\n",
    "tokenizer = AutoTokenizer.from_pretrained( \"google/gemma-2b\", token=os.environ['HF_TOKEN'])\n",
    "len(tokenizer.encode(answer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
